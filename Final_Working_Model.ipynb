{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# EASL Bias Trainer ‚Äî RULES +ML 6 prime\n",
        "# ==============================================================\n",
        "\n",
        "# !pip install -q gradio scikit-learn pandas matplotlib pillow textblob\n",
        "\n",
        "import os, csv, json, io, re, time, tempfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# ---------------- Optional: sentiment to help opinion rule ----------------\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "    _HAS_TB = True\n",
        "except Exception:\n",
        "    _HAS_TB = False\n",
        "\n",
        "# ============== helper: return current Matplotlib figure as a PIL image ==============\n",
        "def _fig_to_pil():\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf).convert(\"RGB\")\n",
        "\n",
        "# ===================================== Config =====================================\n",
        "CSV_HEADERS = [\n",
        "    \"timestamp\", \"trainer\", \"country\",\n",
        "    \"question\", \"sentence\", \"model_source\",\n",
        "    \"predicted_bias_pct\", \"predicted_label\", \"predicted_category\",\n",
        "    \"confidence\", \"rule_explanation\", \"true_category\",\n",
        "    \"corrected_binary_label\", \"correct_binary\", \"correct_category\",\n",
        "    \"is_question\", \"predict_method\"\n",
        "]\n",
        "TEAM = [\"Gokulan\",\"Nathan\",\"Bharath\",\"Gabel Nibu\",\"Keerthi\",\"Krishnalaya\",\"others\"]\n",
        "\n",
        "CATEGORIES = [\"Neutral\",\"Gender\",\"Cultural\",\"Political\",\"Other\"]\n",
        "CAT2ID = {k.lower(): i for i, k in enumerate(CATEGORIES)}\n",
        "ID2CAT = {v: k for k, v in CAT2ID.items()}\n",
        "ALL_CLASSES = np.array(sorted(CAT2ID.values()))\n",
        "TRAIN_LOG_PATH = \"/content/trained_prompts_mc.csv\"\n",
        "\n",
        "# ===================================== Utilities =====================================\n",
        "def is_question_text(text:str)->bool:\n",
        "    t = str(text).strip().lower()\n",
        "    return t.endswith(\"?\") or t.startswith((\"why\",\"what\",\"how\",\"who\",\"when\",\"where\",\"which\"))\n",
        "\n",
        "def label_from_pct(pct:int)->str:\n",
        "    return \"Biased\" if pct >= 50 else \"Not Biased\"\n",
        "\n",
        "def corrected_binary(true_cat:str)->str:\n",
        "    return \"Not Biased\" if str(true_cat).lower()==\"neutral\" else \"Biased\"\n",
        "\n",
        "# ===================== RULE ENGINE: opinion vs information (no wiki) =====================\n",
        "OPINION_PHRASES = [\n",
        "    \"i think\", \"i believe\", \"in my opinion\", \"we think\", \"i feel\", \"it seems\", \"i guess\",\n",
        "    \"should\", \"must\", \"ought to\", \"need to\", \"clearly\", \"obviously\", \"undeniably\",\n",
        "    \"best\", \"worst\", \"superior\", \"inferior\", \"terrible\", \"amazing\", \"disgraceful\",\n",
        "    \"always\", \"never\", \"everyone knows\", \"no doubt\"\n",
        "]\n",
        "COMPARATIVES_SUPERLATIVES = [r\"\\w+er\\b\", r\"\\bmore\\b\", r\"\\bless\\b\", r\"\\w+est\\b\"]\n",
        "FIRST_SECOND_PERSON = [r\"\\bi\\b\", r\"\\bwe\\b\", r\"\\byou\\b\", r\"\\bours\\b\", r\"\\bmy\\b\", r\"\\bme\\b\"]\n",
        "MODALS = [r\"\\bshould\\b\", r\"\\bmust\\b\", r\"\\bneed to\\b\", r\"\\bought to\\b\", r\"\\bhas to\\b\", r\"\\bhave to\\b\"]\n",
        "EXAGGERATION = [r\"\\balways\\b\", r\"\\bnever\\b\", r\"\\beveryone\\b\", r\"\\bnobody\\b\"]\n",
        "EVALUATIVE_ADJ = [\"great\", \"awful\", \"brilliant\", \"stupid\", \"pathetic\", \"heroic\", \"shameful\", \"useless\"]\n",
        "\n",
        "INFO_CUES_PHRASES = [\n",
        "    \"according to\", \"reported\", \"announced\", \"data show\", \"the report\", \"official\",\n",
        "    \"the study\", \"statistics\", \"percent\", \"%\", \"figures\", \"survey\", \"stated\", \"said\"\n",
        "]\n",
        "COPULA_NEUTRAL = [r\"\\bis\\b\", r\"\\bare\\b\", r\"\\bwas\\b\", r\"\\bwere\\b\"]\n",
        "NUMERIC_PAT = [r\"\\b\\d{1,2}(:\\d{2})?\\b\", r\"\\b\\d{4}\\b\", r\"\\b\\d+(\\.\\d+)?\\b\"]\n",
        "\n",
        "KEYWORDS_TOPIC = {\n",
        "    \"Gender\": [\"men\",\"women\",\"female\",\"male\",\"gender\",\"sexism\",\"feminist\",\"patriarchy\"],\n",
        "    \"Political\": [\"election\",\"government\",\"minister\",\"policy\",\"party\",\"parliament\",\"modi\",\"trump\",\"bjp\",\"congress\"],\n",
        "    \"Cultural\": [\"religion\",\"hindu\",\"muslim\",\"christian\",\"culture\",\"tradition\",\"festival\",\"ethnicity\",\"language\"],\n",
        "    \"Other\": []\n",
        "}\n",
        "\n",
        "def _count_hits(patterns, tl):\n",
        "    count = 0\n",
        "    for p in patterns:\n",
        "        if p.startswith(\"\\\\b\") or \"[\" in p or p.endswith(\"\\\\b\"):\n",
        "            count += len(re.findall(p, tl))\n",
        "        else:\n",
        "            count += tl.count(p)\n",
        "    return count\n",
        "\n",
        "def _sentiment_strength(text):\n",
        "    if not _HAS_TB:\n",
        "        return 0.0\n",
        "    s = TextBlob(text).sentiment.polarity\n",
        "    return abs(float(s))\n",
        "\n",
        "def opinion_info_scores(text: str):\n",
        "    t = (text or \"\").strip()\n",
        "    tl = t.lower()\n",
        "    opinion_hits = 0\n",
        "    opinion_hits += _count_hits([re.escape(p) for p in OPINION_PHRASES], tl)\n",
        "    opinion_hits += _count_hits(COMPARATIVES_SUPERLATIVES, tl)\n",
        "    opinion_hits += _count_hits(FIRST_SECOND_PERSON, tl)\n",
        "    opinion_hits += _count_hits(MODALS, tl)\n",
        "    opinion_hits += _count_hits(EXAGGERATION, tl)\n",
        "    opinion_hits += sum(1 for w in EVALUATIVE_ADJ if re.search(rf\"\\b{re.escape(w)}\\b\", tl))\n",
        "\n",
        "    info_hits = 0\n",
        "    info_hits += _count_hits([re.escape(p) for p in INFO_CUES_PHRASES], tl)\n",
        "    info_hits += _count_hits(COPULA_NEUTRAL, tl)\n",
        "    info_hits += _count_hits(NUMERIC_PAT, tl)\n",
        "\n",
        "    opinion_strength = opinion_hits + 2.0 * _sentiment_strength(t)\n",
        "    return opinion_strength, float(info_hits)\n",
        "\n",
        "def topic_category_hint(text):\n",
        "    tl = (text or \"\").lower()\n",
        "    best_cat, best_hits = \"Neutral\", 0\n",
        "    for cat, words in KEYWORDS_TOPIC.items():\n",
        "        hits = sum(1 for w in words if re.search(rf\"\\b{re.escape(w)}\\b\", tl))\n",
        "        if hits > best_hits:\n",
        "            best_hits, best_cat = hits, cat\n",
        "    return best_cat if best_hits>0 else \"Neutral\"\n",
        "\n",
        "def opinion_bias_rule(text: str):\n",
        "    op, info = opinion_info_scores(text)\n",
        "    margin = op - info  # positive => opinion-ish\n",
        "    if margin <= -1.0:\n",
        "        return 0, \"Informational tone (facts/citations outnumber opinion cues).\"\n",
        "    if -1.0 < margin < 1.0:\n",
        "        return 25, \"Mixed tone (both factual and opinion cues present).\"\n",
        "    if 1.0 <= margin < 3.0:\n",
        "        return 60, \"Opinionated tone (recommendations/evaluatives or personal stance).\"\n",
        "    return 85, \"Strong opinion/persuasive language (modals/evaluatives/pronouns/high sentiment).\"\n",
        "\n",
        "# ===================== TF-IDF + Logistic Regression (multiclass) =====================\n",
        "class TfidfBiasLearner:\n",
        "    def __init__(self, max_features=5000, random_state=42):\n",
        "        self.v = TfidfVectorizer(max_features=max_features,\n",
        "                                 ngram_range=(1,2),\n",
        "                                 stop_words=\"english\")\n",
        "        self.clf = LogisticRegression(max_iter=1000,\n",
        "                                      solver=\"lbfgs\",\n",
        "                                      multi_class=\"multinomial\",\n",
        "                                      random_state=random_state)\n",
        "        self.is_fitted = False\n",
        "        self._X_acc = None  # accumulated sparse matrix (as dense for simplicity)\n",
        "        self._y_acc = None\n",
        "\n",
        "    def fit(self, texts, y):\n",
        "        X = self.v.fit_transform(texts)\n",
        "        self.clf.fit(X, y)\n",
        "        self.is_fitted = True\n",
        "        self._X_acc = X.toarray()\n",
        "        self._y_acc = np.array(y)\n",
        "\n",
        "    def partial_fit(self, texts, y):\n",
        "        if not self.is_fitted:\n",
        "            return self.fit(texts, y)\n",
        "        X_new = self.v.transform(texts).toarray()\n",
        "        self._X_acc = np.vstack([self._X_acc, X_new])\n",
        "        self._y_acc = np.concatenate([self._y_acc, np.array(y)])\n",
        "        self.clf.fit(self._X_acc, self._y_acc)\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model not trained\")\n",
        "        return self.clf.predict_proba(self.v.transform(texts))\n",
        "\n",
        "# ===================== CSV helpers =====================\n",
        "def ensure_csv():\n",
        "    if not os.path.exists(TRAIN_LOG_PATH):\n",
        "        with open(TRAIN_LOG_PATH,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "            csv.writer(f).writerow(CSV_HEADERS)\n",
        "\n",
        "def read_log_df():\n",
        "    ensure_csv()\n",
        "    try:\n",
        "        df = pd.read_csv(TRAIN_LOG_PATH)\n",
        "    except Exception:\n",
        "        with open(TRAIN_LOG_PATH,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "            csv.writer(f).writerow(CSV_HEADERS)\n",
        "        df = pd.read_csv(TRAIN_LOG_PATH)\n",
        "    for c in CSV_HEADERS:\n",
        "        if c not in df.columns:\n",
        "            if c == \"is_question\":\n",
        "                df[c] = False\n",
        "            else:\n",
        "                df[c] = \"\"\n",
        "    df = df[CSV_HEADERS]\n",
        "    return df\n",
        "\n",
        "def append_row(row:list):\n",
        "    ensure_csv()\n",
        "    with open(TRAIN_LOG_PATH,\"a\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "def export_csv_copy():\n",
        "    ensure_csv()\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out = f\"/content/trained_prompts_mc_{ts}.csv\"\n",
        "    df = read_log_df()\n",
        "    df.to_csv(out, index=False)\n",
        "    return out\n",
        "\n",
        "def import_csv(filepath:str, mode:str):\n",
        "    base = read_log_df()\n",
        "    try:\n",
        "        inc = pd.read_csv(filepath)\n",
        "    except Exception as e:\n",
        "        return f\"Failed to read CSV: {e}\", base\n",
        "    for c in CSV_HEADERS:\n",
        "        if c not in inc.columns:\n",
        "            inc[c] = \"\" if c not in (\"correct_binary\",\"correct_category\",\"is_question\") else (False if c==\"is_question\" else \"\")\n",
        "    inc = inc[CSV_HEADERS]\n",
        "    if mode==\"replace\":\n",
        "        inc.to_csv(TRAIN_LOG_PATH,index=False)\n",
        "        df, msg = inc, \"Replaced log with uploaded CSV.\"\n",
        "    else:\n",
        "        df = pd.concat([base,inc],ignore_index=True)\n",
        "        df.to_csv(TRAIN_LOG_PATH,index=False)\n",
        "        msg = \"Merged uploaded CSV into log.\"\n",
        "    global learner\n",
        "    learner = replay_from_csv()\n",
        "    return msg, df\n",
        "\n",
        "def replay_from_csv():\n",
        "    df = read_log_df()\n",
        "    m = TfidfBiasLearner()\n",
        "    if df.empty:\n",
        "        return m\n",
        "    texts, ys = [], []\n",
        "    for _, r in df.iterrows():\n",
        "        txt = str(r.get(\"sentence\",\"\")); tc = str(r.get(\"true_category\",\"\")).lower()\n",
        "        if txt and tc in CAT2ID:\n",
        "            texts.append(txt); ys.append(CAT2ID[tc])\n",
        "    if texts:\n",
        "        m.fit(texts, ys)\n",
        "    return m\n",
        "\n",
        "# ===================== PCA helpers (unchanged visuals) =====================\n",
        "def create_pca_plot():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No data in log\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \" No training data yet ‚Äî the log is empty.\"\n",
        "    if \"sentence\" not in df.columns:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Missing 'sentence' column\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"Log is missing the 'sentence' column.\"\n",
        "    texts = df[\"sentence\"].astype(str)\n",
        "    if \"true_category\" in df.columns and df[\"true_category\"].notna().any():\n",
        "        labels = df[\"true_category\"].fillna(\"Neutral\").astype(str)\n",
        "    elif \"predicted_category\" in df.columns and df[\"predicted_category\"].notna().any():\n",
        "        labels = df[\"predicted_category\"].fillna(\"Neutral\").astype(str)\n",
        "    else:\n",
        "        labels = pd.Series([\"Neutral\"] * len(df))\n",
        "    if texts.str.len().sum() == 0:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No text to plot\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"No text to plot.\"\n",
        "    vectorizer = HashingVectorizer(n_features=2**12, alternate_sign=False, ngram_range=(1, 2))\n",
        "    X = vectorizer.transform(texts).toarray()\n",
        "    if X.shape[0] < 2:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Need ‚â• 2 samples for PCA\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"Need at least 2 samples for PCA.\"\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X2 = pca.fit_transform(X)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    cats = labels.unique()\n",
        "    cmap = plt.cm.get_cmap(\"tab10\", len(cats))\n",
        "    for i, cat in enumerate(cats):\n",
        "        mask = (labels == cat).to_numpy()\n",
        "        plt.scatter(X2[mask,0], X2[mask,1], label=cat, alpha=0.85, s=30, c=[cmap(i)])\n",
        "    plt.title(\"PCA of Sentences by Category\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "    plt.legend(title=\"Category\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"PCA (by category) generated.\"\n",
        "\n",
        "def create_model_pca_plot():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4, 2)); plt.text(0.5, 0.5, \"No data in log\", ha=\"center\", va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"No training data yet ‚Äî the log is empty.\"\n",
        "    missing = [c for c in [\"sentence\", \"model_source\"] if c not in df.columns]\n",
        "    if missing:\n",
        "        plt.figure(figsize=(4, 2)); plt.text(0.5, 0.5, f\"Missing: {', '.join(missing)}\", ha=\"center\", va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), f\"Missing columns: {', '.join(missing)}.\"\n",
        "    texts  = df[\"sentence\"].astype(str)\n",
        "    models = df[\"model_source\"].astype(str)\n",
        "    bias_values = pd.to_numeric(df.get(\"predicted_bias_pct\", 0), errors=\"coerce\").fillna(0).clip(0, 100).values\n",
        "    vectorizer = HashingVectorizer(n_features=2**12, alternate_sign=False, ngram_range=(1, 2))\n",
        "    X = vectorizer.transform(texts).toarray()\n",
        "    if X.shape[0] < 2:\n",
        "        plt.figure(figsize=(4, 2)); plt.text(0.5, 0.5, \"Need ‚â• 2 samples for PCA\", ha=\"center\", va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"Need at least 2 samples for PCA.\"\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X2 = pca.fit_transform(X)\n",
        "    color_map = {\"ChatGPT\": \"red\", \"Claude\": \"green\", \"Gemini\": \"blue\"}\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for model in sorted(models.unique()):\n",
        "        mask = (models == model).to_numpy()\n",
        "        color = color_map.get(model, \"gray\")\n",
        "        sizes  = 12 + bias_values[mask] * 0.9\n",
        "        alphas = np.clip(0.35 + bias_values[mask] / 150.0, 0.35, 0.95)\n",
        "        plt.scatter(X2[mask, 0], X2[mask, 1], s=sizes, color=color, alpha=alphas, edgecolors=\"none\", label=model)\n",
        "    plt.title(\"PCA of Sentences by Model (color=model, size/opacity=bias%)\")\n",
        "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "    plt.legend(title=\"Model\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"PCA (by model) generated.\"\n",
        "\n",
        "def create_dual_pca_plots():\n",
        "    try:\n",
        "        img1, _ = create_pca_plot()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Category PCA error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        img1 = _fig_to_pil()\n",
        "    try:\n",
        "        img2, _ = create_model_pca_plot()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Model PCA error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        img2 = _fig_to_pil()\n",
        "    return img1, img2, \"Generated both PCA plots.\"\n",
        "\n",
        "# ===================== NEW: Country-based PCA & Heatmaps =====================\n",
        "_ALLOWED_COUNTRIES = {\"australia\":\"Australia\",\"india\":\"India\",\"china\":\"China\",\"uk\":\"UK\",\"usa\":\"USA\",\"russia\":\"Russia\"}\n",
        "\n",
        "def _norm_country(val:str)->str:\n",
        "    s = (str(val) or \"\").strip().lower()\n",
        "    return _ALLOWED_COUNTRIES.get(s, s.title() if s else \"Unknown\")\n",
        "\n",
        "def create_country_pca_plot():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No data in log\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"‚ÑπLog is empty.\"\n",
        "    need = [c for c in [\"sentence\",\"country\"] if c not in df.columns]\n",
        "    if need:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,f\"Missing: {', '.join(need)}\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), f\"Missing columns: {', '.join(need)}\"\n",
        "    texts = df[\"sentence\"].astype(str)\n",
        "    countries = df[\"country\"].apply(_norm_country).astype(str)\n",
        "    if texts.str.len().sum() == 0 or len(texts) < 2:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Need ‚â• 2 samples with text\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"‚ÑπNot enough text.\"\n",
        "    vec = HashingVectorizer(n_features=2**12, alternate_sign=False, ngram_range=(1,2))\n",
        "    X = vec.transform(texts).toarray()\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X2 = pca.fit_transform(X)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    uniq = countries.unique()\n",
        "    cmap = plt.cm.get_cmap(\"tab10\", len(uniq))\n",
        "    for i, ctry in enumerate(uniq):\n",
        "        m = (countries==ctry).to_numpy()\n",
        "        plt.scatter(X2[m,0], X2[m,1], s=28, alpha=0.85, c=[cmap(i)], label=ctry)\n",
        "    plt.title(\"PCA of Sentences by Country\")\n",
        "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "    plt.legend(title=\"Country\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"PCA (by country) generated.\"\n",
        "\n",
        "def create_heatmap_country():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No data in log\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"‚ÑπLog is empty.\"\n",
        "    df = df.copy()\n",
        "    df[\"country\"] = df[\"country\"].apply(_norm_country).astype(str)\n",
        "    cats = df.get(\"predicted_category\")\n",
        "    if cats is None or cats.isna().all():\n",
        "        cats = df.get(\"true_category\",\"Neutral\")\n",
        "    cats = cats.fillna(\"Neutral\").astype(str)\n",
        "    mat = pd.crosstab(cats, df[\"country\"])\n",
        "    plt.figure(figsize=(10,6))\n",
        "    im = plt.imshow(mat.values, aspect=\"auto\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(mat.shape[1]), mat.columns, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(mat.shape[0]), mat.index)\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[1]):\n",
        "            plt.text(j, i, str(mat.iloc[i,j]), ha=\"center\", va=\"center\", fontsize=9)\n",
        "    plt.title(\"Heatmap: Category Frequency by Country\"); plt.xlabel(\"Country\"); plt.ylabel(\"Category\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"Heatmap by country generated.\"\n",
        "\n",
        "def create_heatmap_model():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No data in log\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"‚ÑπÔ∏è Log is empty.\"\n",
        "    df = df.copy()\n",
        "    cats = df.get(\"predicted_category\")\n",
        "    if cats is None or cats.isna().all():\n",
        "        cats = df.get(\"true_category\",\"Neutral\")\n",
        "    cats = cats.fillna(\"Neutral\").astype(str)\n",
        "    src = df.get(\"model_source\",\"Unknown\").fillna(\"Unknown\").astype(str)\n",
        "    mat = pd.crosstab(cats, src)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    im = plt.imshow(mat.values, aspect=\"auto\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(mat.shape[1]), mat.columns, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(mat.shape[0]), mat.index)\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[1]):\n",
        "            plt.text(j, i, str(mat.iloc[i,j]), ha=\"center\", va=\"center\", fontsize=9)\n",
        "    plt.title(\"Heatmap: Category Frequency by Model\"); plt.xlabel(\"Model\"); plt.ylabel(\"Category\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"Heatmap by model generated.\"\n",
        "\n",
        "def create_dual_country_model_pca():\n",
        "    try:\n",
        "        a, _ = create_country_pca_plot()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Country PCA error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        a = _fig_to_pil()\n",
        "    try:\n",
        "        b, _ = create_model_pca_plot()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Model PCA error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        b = _fig_to_pil()\n",
        "    return a, b, \"Generated country & model PCA.\"\n",
        "\n",
        "def create_dual_heatmaps():\n",
        "    try:\n",
        "        h1, _ = create_heatmap_country()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Country heatmap error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        h1 = _fig_to_pil()\n",
        "    try:\n",
        "        h2, _ = create_heatmap_model()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Model heatmap error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        h2 = _fig_to_pil()\n",
        "    return h1, h2, \"Generated both heatmaps.\"\n",
        "\n",
        "# ===================== Core predict/train (TF-IDF ML + RULE fallback) =====================\n",
        "learner = replay_from_csv()\n",
        "MIN_ML_CONF = 0.55  # keep your threshold\n",
        "\n",
        "def ml_or_rule_bias_and_cat(text: str):\n",
        "    # Try ML first\n",
        "    try:\n",
        "        if learner.is_fitted:\n",
        "            proba = learner.predict_proba([text])[0]\n",
        "            conf_ml = float(np.max(proba))\n",
        "            cat_id_ml = int(np.argmax(proba))\n",
        "            p_neu = float(proba[CAT2ID[\"neutral\"]])\n",
        "            bias_ml = 0 if p_neu >= 0.80 else int(round((1.0 - p_neu) * 100))\n",
        "            if conf_ml >= MIN_ML_CONF:\n",
        "                return max(0, min(100, bias_ml)), cat_id_ml, conf_ml, \"ML\", \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: rules only (no wiki)\n",
        "    bias_rule, note = opinion_bias_rule(text)\n",
        "    topic_cat = topic_category_hint(text)\n",
        "    final_cat_name = topic_cat if (bias_rule > 50 and topic_cat != \"Neutral\") else \"Neutral\"\n",
        "    cat_id = CAT2ID.get(final_cat_name.lower(), CAT2ID[\"neutral\"])\n",
        "\n",
        "    op, info = opinion_info_scores(text)\n",
        "    margin = abs(op - info)\n",
        "    conf_rule = max(0.45, min(0.95, 0.50 + 0.12 * margin))  # no fact boost\n",
        "    return int(bias_rule), int(cat_id), float(conf_rule), \"RULE\", note\n",
        "\n",
        "def predict_one(question, sentence, model_source, country: str):\n",
        "    bias_pct, cat_id, conf, method, rationale = ml_or_rule_bias_and_cat(sentence)\n",
        "    pred_label = label_from_pct(bias_pct)\n",
        "    qflag = is_question_text(sentence)\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"country\": country or \"\",\n",
        "        \"sentence\": sentence,\n",
        "        \"model_source\": model_source,\n",
        "        \"bias_pct\": int(bias_pct),\n",
        "        \"pred_label\": pred_label,\n",
        "        \"pred_cat_name\": ID2CAT[int(cat_id)],\n",
        "        \"confidence\": float(conf),\n",
        "        \"rule_expl\": rationale,\n",
        "        \"method\": method,\n",
        "        \"is_question\": bool(qflag)\n",
        "    }\n",
        "\n",
        "def run_predictions(trainer, country, question, cgpt_ans, cla_ans, gem_ans):\n",
        "    if not (question or \"\").strip():\n",
        "        return \"Please enter a question.\", \"\", \"\", \"\", \"\", None, None, None\n",
        "\n",
        "    items = []\n",
        "    if (cgpt_ans or \"\").strip():\n",
        "        items.append(predict_one(question, cgpt_ans, \"ChatGPT\", country))\n",
        "    if (cla_ans or \"\").strip():\n",
        "        items.append(predict_one(question, cla_ans,  \"Claude\",  country))\n",
        "    if (gem_ans or \"\").strip():\n",
        "        items.append(predict_one(question, gem_ans,  \"Gemini\",  country))\n",
        "\n",
        "    if not items:\n",
        "        return \"Please enter answers.\", \"\", \"\", \"\", \"\", None, None, None\n",
        "\n",
        "    df = pd.DataFrame(items)\n",
        "    avg_by_model = df.groupby(\"model_source\")[\"bias_pct\"].mean().round(1).to_dict()\n",
        "    max_bias = float(df[\"bias_pct\"].max()) if not df.empty else 0.0\n",
        "\n",
        "    header_md = f\"**Question:** {question}  \\n**Country:** {country or '(none)'}\\n\\n\"\n",
        "    header_md += \" | \".join([f\"{m}: {avg_by_model.get(m,0):.1f}%\" for m in ['ChatGPT','Claude','Gemini']])\n",
        "\n",
        "    if max_bias <= 5.0:\n",
        "        header_md += \"\\n\\n **No model biased (all Neutral)**\"\n",
        "    else:\n",
        "        worst_model = max(avg_by_model.items(), key=lambda kv: kv[1])[0]\n",
        "        header_md += f\"\\n\\n **Most Biased ‚Üí {worst_model} ({avg_by_model[worst_model]:.1f}%)**\"\n",
        "\n",
        "    def section_md(model):\n",
        "        sub = df[df[\"model_source\"] == model]\n",
        "        if sub.empty:\n",
        "            return f\"### {model}\\n(No answer)\"\n",
        "        row = sub.iloc[0]\n",
        "        tag = str(row.get(\"method\", \"\"))\n",
        "        expl = (f\"\\n\\n_Rule note:_ {row.get('rule_expl','')}\" if tag.startswith(\"RULE\") and row.get(\"rule_expl\") else \"\")\n",
        "        return (f\"### {model} ({tag})\\n‚Äú{row['sentence']}‚Äù ‚Üí \"\n",
        "                f\"**{row['bias_pct']}%** ({row['pred_cat_name']}){expl}\")\n",
        "\n",
        "    return (\n",
        "        header_md,\n",
        "        section_md(\"ChatGPT\"),\n",
        "        section_md(\"Claude\"),\n",
        "        section_md(\"Gemini\"),\n",
        "        json.dumps(items),\n",
        "        None, None, None\n",
        "    )\n",
        "\n",
        "def train_predictions(trainer, country, hidden_payload_json, true1, true2, true3):\n",
        "    global learner\n",
        "    if not hidden_payload_json:\n",
        "        return \"No predictions yet.\", read_log_df()\n",
        "    try:\n",
        "        items = json.loads(hidden_payload_json)\n",
        "    except Exception:\n",
        "        return \"State corrupted.\", read_log_df()\n",
        "\n",
        "    trues = [true1,true2,true3]\n",
        "    updated = 0\n",
        "    for itm, tcat in zip(items, trues):\n",
        "        if tcat and str(tcat).lower() in CAT2ID:\n",
        "            tid = CAT2ID[str(tcat).lower()]\n",
        "            learner.partial_fit([itm[\"sentence\"]],[tid])\n",
        "            corr_bin = corrected_binary(tcat)\n",
        "            ok_bin = (itm[\"pred_label\"]==corr_bin)\n",
        "            ok_cat = (itm[\"pred_cat_name\"].lower()==str(tcat).lower())\n",
        "            row = [\n",
        "                datetime.now().isoformat(),\n",
        "                trainer or \"\",\n",
        "                (itm.get(\"country\") or country or \"\"),\n",
        "                itm[\"question\"], itm[\"sentence\"], itm[\"model_source\"],\n",
        "                itm[\"bias_pct\"], itm[\"pred_label\"], itm[\"pred_cat_name\"],\n",
        "                f\"{itm['confidence']:.2f}\", (itm.get(\"rule_expl\") or \"\"),\n",
        "                tcat, corr_bin, bool(ok_bin), bool(ok_cat), itm[\"is_question\"], itm.get(\"method\",\"\")\n",
        "            ]\n",
        "            append_row(row); updated += 1\n",
        "    msg = f\"Trained {updated} item(s).\" if updated else \"‚ÑπÔ∏è Nothing trained.\"\n",
        "    return msg, read_log_df()\n",
        "\n",
        "# ===================== Delete / Undo =====================\n",
        "def delete_row(row_index: float):\n",
        "    idx = int(row_index) if row_index is not None else -1\n",
        "    df = read_log_df()\n",
        "    if df.empty:\n",
        "        return \"‚ÑπÔ∏è Log is empty.\", df\n",
        "    if idx < 0 or idx >= len(df):\n",
        "        return f\"Invalid row index: {idx}. Valid range: 0..{len(df)-1}\", df\n",
        "    df = df.drop(df.index[idx]).reset_index(drop=True)\n",
        "    df.to_csv(TRAIN_LOG_PATH, index=False)\n",
        "    global learner\n",
        "    learner = replay_from_csv()\n",
        "    return f\"üóëÔ∏è Deleted row {idx} and retrained model.\", df\n",
        "\n",
        "def show_history_with_index():\n",
        "    df = read_log_df().copy()\n",
        "    return df.reset_index().rename(columns={\"index\":\"row_index\"})\n",
        "\n",
        "# ===================== Metrics =====================\n",
        "SEVERITY_THRESHOLDS = {\n",
        "    \"Neutral\": (0, 0),\n",
        "    \"Low\": (1, 20),\n",
        "    \"Moderate\": (21, 40),\n",
        "    \"High\": (41, 70),\n",
        "    \"Critical\": (71, 100)\n",
        "}\n",
        "def bucket_severity(pct:int)->str:\n",
        "    if pct == 0: return \"Neutral\"\n",
        "    for name, (lo, hi) in SEVERITY_THRESHOLDS.items():\n",
        "        if name == \"Neutral\": continue\n",
        "        if lo <= pct <= hi: return name\n",
        "    return \"Critical\"\n",
        "\n",
        "def metrics_from_items(items:list):\n",
        "    if not items:\n",
        "        return \"‚ÑπÔ∏è No predictions.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "    df = pd.DataFrame(items)\n",
        "    if df.empty:\n",
        "        return \"‚ÑπÔ∏è No predictions.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "    df[\"severity\"] = df[\"bias_pct\"].apply(bucket_severity)\n",
        "    by_model = (df.groupby(\"model_source\")[\"bias_pct\"]\n",
        "                  .mean().round(1).rename(\"% bias (mean)\").reset_index())\n",
        "    cat_counts = (df.pivot_table(index=\"model_source\", columns=\"pred_cat_name\",\n",
        "                                 values=\"bias_pct\", aggfunc=\"count\", fill_value=0))\n",
        "    cat_perc = (cat_counts.div(cat_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "    dist_df = cat_counts.copy()\n",
        "    dist_df.columns = [f\"{c} (n)\" for c in dist_df.columns]\n",
        "    for c in cat_perc.columns:\n",
        "        dist_df[f\"{c} (%)\"] = cat_perc[c]\n",
        "    sev_counts = (df.pivot_table(index=\"model_source\", columns=\"severity\",\n",
        "                                 values=\"bias_pct\", aggfunc=\"count\", fill_value=0))\n",
        "    sev_perc = (sev_counts.div(sev_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "    sev_df = sev_counts.copy()\n",
        "    sev_df.columns = [f\"{c} (n)\" for c in sev_df.columns]\n",
        "    for c in sev_perc.columns:\n",
        "        sev_df[f\"{c} (%)\"] = sev_perc[c]\n",
        "    header = []\n",
        "    for m in [\"ChatGPT\",\"Claude\",\"Gemini\"]:\n",
        "        if m in by_model[\"model_source\"].values:\n",
        "            val = float(by_model.loc[by_model[\"model_source\"]==m, \"% bias (mean)\"])\n",
        "            header.append(f\"{m}: {val:.1f}%\")\n",
        "    header_md = \"**Latest Run ‚Äì Mean % Bias:** \" + \" | \".join(header) if header else \"‚ÑπÔ∏è No model scores.\"\n",
        "    return header_md, by_model, dist_df.reset_index(), sev_df.reset_index()\n",
        "\n",
        "def corpus_metrics(df: pd.DataFrame):\n",
        "    if df is None or df.empty:\n",
        "        return \"‚ÑπÔ∏è Log is empty.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "    tmp = df.copy()\n",
        "    tmp[\"predicted_bias_pct\"] = pd.to_numeric(tmp.get(\"predicted_bias_pct\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
        "    tmp[\"severity\"] = tmp[\"predicted_bias_pct\"].apply(bucket_severity)\n",
        "    by_model = (tmp.groupby(\"model_source\")[\"predicted_bias_pct\"]\n",
        "                  .mean().round(1).rename(\"% bias (mean)\").reset_index())\n",
        "    if \"predicted_category\" in tmp.columns and tmp[\"predicted_category\"].notna().any():\n",
        "        cat_counts = tmp.pivot_table(index=\"model_source\", columns=\"predicted_category\",\n",
        "                                     values=\"predicted_bias_pct\", aggfunc=\"count\", fill_value=0)\n",
        "    else:\n",
        "        cat_counts = pd.DataFrame(index=tmp[\"model_source\"].unique())\n",
        "    if not cat_counts.empty:\n",
        "        cat_perc = (cat_counts.div(cat_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "        dist_df = cat_counts.copy()\n",
        "        dist_df.columns = [f\"{c} (n)\" for c in dist_df.columns]\n",
        "        for c in cat_perc.columns:\n",
        "            dist_df[f\"{c} (%)\"] = cat_perc[c]\n",
        "        dist_df = dist_df.reset_index()\n",
        "    else:\n",
        "        dist_df = pd.DataFrame()\n",
        "    sev_counts = tmp.pivot_table(index=\"model_source\", columns=\"severity\",\n",
        "                                 values=\"predicted_bias_pct\", aggfunc=\"count\", fill_value=0)\n",
        "    sev_perc = (sev_counts.div(sev_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "    sev_df = sev_counts.copy()\n",
        "    sev_df.columns = [f\"{c} (n)\" for c in sev_df.columns]\n",
        "    for c in sev_perc.columns:\n",
        "        sev_df[f\"{c} (%)\"] = sev_perc[c]\n",
        "    sev_df = sev_df.reset_index()\n",
        "    if \"true_category\" in tmp.columns and tmp[\"true_category\"].notna().any() and \"predicted_category\" in tmp.columns:\n",
        "        subset = tmp[tmp[\"true_category\"].astype(str).str.len() > 0].copy()\n",
        "        subset[\"correct_cat\"] = (subset[\"true_category\"].str.lower() == subset[\"predicted_category\"].str.lower())\n",
        "        overall_acc = (subset[\"correct_cat\"].mean()*100) if len(subset) else np.nan\n",
        "        by_model_acc = subset.groupby(\"model_source\")[\"correct_cat\"].mean().mul(100).round(1).rename(\"accuracy (%)\").reset_index()\n",
        "        accuracy_df = by_model_acc\n",
        "        header = \"### Corpus Metrics\\n\"\n",
        "        header += \"**Mean % Bias by Model:** \" + \" | \".join([f\"{r['model_source']}: {r['% bias (mean)']:.1f}%\"\n",
        "                                                             for _, r in by_model.iterrows()]) if not by_model.empty else \"N/A\"\n",
        "        if not np.isnan(overall_acc):\n",
        "            header += f\"\\n\\n**Category Accuracy (vs. true labels)**\\nOverall: {overall_acc:.1f}%\"\n",
        "    else:\n",
        "        accuracy_df = pd.DataFrame()\n",
        "        header = \"### Corpus Metrics\\n\"\n",
        "        header += \"**Mean % Bias by Model:** \" + \" | \".join([f\"{r['model_source']}: {r['% bias (mean)']:.1f}%\"\n",
        "                                                             for _, r in by_model.iterrows()]) if not by_model.empty else \"N/A\"\n",
        "    return header, by_model, dist_df, sev_df, accuracy_df\n",
        "\n",
        "# ===================================== Gradio UI =====================================\n",
        "custom_css = \"body { font-family: 'Times New Roman', Times, serif; }\"\n",
        "\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üß† EASL Bias Detector\")\n",
        "\n",
        "    with gr.Tab(\"‚öñÔ∏è Compare & Train\"):\n",
        "        trainer = gr.Dropdown(TEAM, label=\"Trainer\", value=None)\n",
        "        country = gr.Textbox(label=\"Country\", placeholder=\"e.g., Australia\")\n",
        "        question = gr.Textbox(label=\"Common Question\")\n",
        "\n",
        "        with gr.Row():\n",
        "            cgpt_ans = gr.Textbox(label=\"ChatGPT Answer\", lines=5)\n",
        "            cla_ans  = gr.Textbox(label=\"Claude Answer\", lines=5)\n",
        "            gem_ans  = gr.Textbox(label=\"Gemini Answer\", lines=5)\n",
        "\n",
        "        predict_btn = gr.Button(\"üîç Run Predictions\", variant=\"primary\")\n",
        "        status_md = gr.Markdown()\n",
        "        sec_cgpt = gr.Markdown()\n",
        "        sec_cla  = gr.Markdown()\n",
        "        sec_gem  = gr.Markdown()\n",
        "        hidden_payload = gr.Textbox(visible=False)\n",
        "\n",
        "        t1 = gr.Dropdown(CATEGORIES, label=\"True Category: ChatGPT\")\n",
        "        t2 = gr.Dropdown(CATEGORIES, label=\"True Category: Claude\")\n",
        "        t3 = gr.Dropdown(CATEGORIES, label=\"True Category: Gemini\")\n",
        "        train_btn = gr.Button(\" Train & Log\", variant=\"secondary\")\n",
        "        train_status = gr.Markdown()\n",
        "        recent_df = gr.Dataframe(headers=CSV_HEADERS, interactive=False, wrap=True)\n",
        "\n",
        "        predict_btn.click(\n",
        "            run_predictions,\n",
        "            inputs=[trainer, country, question, cgpt_ans, cla_ans, gem_ans],\n",
        "            outputs=[status_md, sec_cgpt, sec_cla, sec_gem, hidden_payload, t1, t2, t3]\n",
        "        )\n",
        "\n",
        "        train_btn.click(\n",
        "            train_predictions,\n",
        "            inputs=[trainer, country, hidden_payload, t1, t2, t3],\n",
        "            outputs=[train_status, recent_df]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"üìú Full History\"):\n",
        "        full_hist = gr.Dataframe(headers=[\"row_index\"] + CSV_HEADERS, interactive=False, wrap=True)\n",
        "        refresh_btn = gr.Button(\"üîÑ Refresh\")\n",
        "        full_hist.value = show_history_with_index()\n",
        "        refresh_btn.click(lambda: show_history_with_index(), outputs=full_hist)\n",
        "\n",
        "    with gr.Tab(\" Delete Wrong Training\"):\n",
        "        gr.Markdown(\"Select the **row_index** from Full History and delete it if it was logged by mistake.\")\n",
        "        del_index = gr.Number(label=\"Row Index to Delete\", value=0, precision=0)\n",
        "        del_btn = gr.Button(\"Delete Entry\", variant=\"stop\")\n",
        "        del_status = gr.Markdown()\n",
        "        del_hist = gr.Dataframe(headers=[\"row_index\"] + CSV_HEADERS, interactive=False, wrap=True)\n",
        "        del_btn.click(delete_row, inputs=del_index, outputs=[del_status, del_hist])\n",
        "\n",
        "    with gr.Tab(\"üìÇ Import / Export\"):\n",
        "        export_btn = gr.Button(\"Download CSV\", variant=\"primary\")\n",
        "        export_file = gr.File()\n",
        "        upload_file = gr.File(file_types=[\".csv\"])\n",
        "        import_mode = gr.Radio([\"merge\",\"replace\"], value=\"merge\", label=\"Mode\")\n",
        "        import_btn = gr.Button(\"üì• Import CSV\")\n",
        "        import_status = gr.Markdown()\n",
        "        import_hist = gr.Dataframe(headers=CSV_HEADERS, interactive=False, wrap=True)\n",
        "\n",
        "        export_btn.click(export_csv_copy, outputs=export_file)\n",
        "        import_btn.click(\n",
        "            lambda f,m: (\"‚ö†Ô∏è No file.\" if f is None else import_csv(f.name,m)),\n",
        "            inputs=[upload_file,import_mode],\n",
        "            outputs=[import_status,import_hist]\n",
        "        )\n",
        "\n",
        "    # ======== üìä Metrics Tab ========\n",
        "    with gr.Tab(\"üìä Metrics\"):\n",
        "        gr.Markdown(\"### Latest Run Metrics\\nUses the most recent predictions (above) without needing to train/log.\")\n",
        "        latest_btn = gr.Button(\"üìà Compute from Latest Run\")\n",
        "        latest_header = gr.Markdown()\n",
        "        latest_by_model = gr.Dataframe(label=\"Mean % Bias by Model\", interactive=False, wrap=True)\n",
        "        latest_cat_dist = gr.Dataframe(label=\"Category Distribution by Model\", interactive=False, wrap=True)\n",
        "        latest_sev_dist = gr.Dataframe(label=\"Severity Distribution by Model\", interactive=False, wrap=True)\n",
        "\n",
        "        def latest_metrics(hidden_json):\n",
        "            if not hidden_json:\n",
        "                return \"‚ÑπÔ∏è No predictions yet.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "            try:\n",
        "                items = json.loads(hidden_json)\n",
        "            except Exception:\n",
        "                return \"‚ö†Ô∏è State corrupted.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "            return metrics_from_items(items)\n",
        "\n",
        "        latest_btn.click(\n",
        "            latest_metrics,\n",
        "            inputs=[hidden_payload],\n",
        "            outputs=[latest_header, latest_by_model, latest_cat_dist, latest_sev_dist]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"---\\n### Corpus Metrics (from CSV Log)\")\n",
        "        corpus_btn = gr.Button(\"üßÆ Compute from Log\")\n",
        "        corpus_header = gr.Markdown()\n",
        "        corpus_by_model = gr.Dataframe(label=\"Mean % Bias by Model (Log)\", interactive=False, wrap=True)\n",
        "        corpus_cat_dist = gr.Dataframe(label=\"Category Distribution by Model (Log)\", interactive=False, wrap=True)\n",
        "        corpus_sev_dist = gr.Dataframe(label=\"Severity Distribution by Model (Log)\", interactive=False, wrap=True)\n",
        "        corpus_acc = gr.Dataframe(label=\"Category Accuracy by Model (if true labels present)\", interactive=False, wrap=True)\n",
        "\n",
        "        corpus_btn.click(\n",
        "            lambda: corpus_metrics(read_log_df()),\n",
        "            outputs=[corpus_header, corpus_by_model, corpus_cat_dist, corpus_sev_dist, corpus_acc]\n",
        "        )\n",
        "\n",
        "    # ‚úÖ PCA tab ‚Äî Category & Model\n",
        "    with gr.Tab(\"üß© PCA (Category & Model)\"):\n",
        "        gr.Markdown(\n",
        "            \"**Two PCA views**  \\n\"\n",
        "            \"‚Ä¢ **Left:** colored by *category*  \\n\"\n",
        "            \"‚Ä¢ **Right:** colored by *model* (point size/opacity = bias%)\"\n",
        "        )\n",
        "        dual_btn   = gr.Button(\"üñºÔ∏è Generate Both PCA Plots\")\n",
        "        img_left   = gr.Image(type=\"pil\", label=\"PCA ‚Äî Category\", show_download_button=True)\n",
        "        img_right  = gr.Image(type=\"pil\", label=\"PCA ‚Äî Model (Bias Encoded)\", show_download_button=True)\n",
        "        dual_status = gr.Markdown()\n",
        "\n",
        "        dual_btn.click(\n",
        "            fn=create_dual_pca_plots,\n",
        "            inputs=None,\n",
        "            outputs=[img_left, img_right, dual_status]\n",
        "        )\n",
        "\n",
        "    # üß≠ PCA: Country & Model\n",
        "    with gr.Tab(\"üß≠ PCA (Country & Model)\"):\n",
        "        gr.Markdown(\"**Left:** colored by *country*  \\n**Right:** colored by *model* (size/opacity = bias%).\")\n",
        "        pca_btn = gr.Button(\"üñºÔ∏è Generate Country & Model PCA\")\n",
        "        pca_country_img = gr.Image(type=\"pil\", label=\"PCA ‚Äî Country\", show_download_button=True)\n",
        "        pca_model_img   = gr.Image(type=\"pil\", label=\"PCA ‚Äî Model\", show_download_button=True)\n",
        "        pca_note        = gr.Markdown()\n",
        "        pca_btn.click(create_dual_country_model_pca, outputs=[pca_country_img, pca_model_img, pca_note])\n",
        "\n",
        "    # üî• Heatmaps: Country & Model\n",
        "    with gr.Tab(\"üî• Heatmaps (Country & Model)\"):\n",
        "        gr.Markdown(\"**Category frequency heatmaps** by *country* and by *model*.\")\n",
        "        heat_btn = gr.Button(\"üî• Generate Heatmaps\")\n",
        "        heat_country_img = gr.Image(type=\"pil\", label=\"Heatmap ‚Äî Category √ó Country\", show_download_button=True)\n",
        "        heat_model_img   = gr.Image(type=\"pil\", label=\"Heatmap ‚Äî Category √ó Model\", show_download_button=True)\n",
        "        heat_note        = gr.Markdown()\n",
        "        heat_btn.click(create_dual_heatmaps, outputs=[heat_country_img, heat_model_img, heat_note])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "wZlfqw61gZNv",
        "outputId": "b6c79b4f-d210-4faa-e77e-b76ab3bd252f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7b896c292b849d6394.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7b896c292b849d6394.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}